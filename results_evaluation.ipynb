{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/michielkorpel/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michielkorpel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michielkorpel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/michielkorpel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/michielkorpel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# nltk\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others\n",
    "import re, string, random\n",
    "import pandas as pd\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_candidate(tweet_text, in_reply_to, quoted_status):\n",
    "    # Find mentions and hashtags in tweet_text\n",
    "    trump_mentions = re.findall('@realDonaldTrump', tweet_text, re.IGNORECASE)\n",
    "    clinton_mentions = re.findall('@HillaryClinton', tweet_text, re.IGNORECASE)\n",
    "    mentioned_first = re.findall('@realDonaldTrump|@HillaryClinton', tweet_text, re.IGNORECASE)\n",
    "    trump_hashtags = re.findall('#maga|#trumppence16|#donaldtrump|#trump|#dumptrump|#nevertrump', tweet_text, re.IGNORECASE)\n",
    "    clinton_hashtags = re.findall('#hillaryclinton|#hillary|#imwithher|#crookedhillary|#neverhillary', tweet_text, re.IGNORECASE)\n",
    "    # Find mentions and hashtags in quoted_status.text\n",
    "    if isinstance(quoted_status, dict):\n",
    "        quoted_status_text = quoted_status['text']\n",
    "        trump_mentions.extend(re.findall('@realDonaldTrump', quoted_status_text, re.IGNORECASE))\n",
    "        clinton_mentions.extend(re.findall('@HillaryClinton', quoted_status_text, re.IGNORECASE))\n",
    "        mentioned_first.extend(re.findall('@realDonaldTrump|@HillaryClinton', quoted_status_text, re.IGNORECASE))\n",
    "        trump_hashtags.extend(re.findall('#maga|#trumppence16|#donaldtrump|#trump|#dumptrump|#nevertrump', quoted_status_text, re.IGNORECASE))\n",
    "        clinton_hashtags.extend(re.findall('#hillaryclinton|#hillary|#imwithher|#crookedhillary|#neverhillary', quoted_status_text, re.IGNORECASE))\n",
    "    # Find mentions in in_reply_to\n",
    "    if not in_reply_to == None:\n",
    "        trump_mentions.extend(re.findall('realDonaldTrump', in_reply_to, re.IGNORECASE))\n",
    "        clinton_mentions.extend(re.findall('HillaryClinton', in_reply_to, re.IGNORECASE))\n",
    "    # Determine candidate based on most mentions\n",
    "    if len(trump_mentions) == len(clinton_mentions):\n",
    "        # Equal number of mentions, check if one candidate is mentioned first\n",
    "        if not mentioned_first == []:\n",
    "            if mentioned_first[0] == '@realDonaldTrump':\n",
    "                return 'Trump'\n",
    "            else:\n",
    "                return 'Clinton'\n",
    "        # No mentions for either candidate, compare hashtags\n",
    "        if len(trump_hashtags) == len(clinton_hashtags):\n",
    "            return 'Neither'\n",
    "        elif len(trump_hashtags) > len(clinton_hashtags):\n",
    "            return 'Trump'\n",
    "        else:\n",
    "            return 'Clinton'\n",
    "    elif len(trump_mentions) > len(clinton_mentions):\n",
    "        return 'Trump'\n",
    "    else:\n",
    "        return 'Clinton'\n",
    "\n",
    "def remove_ats_hts_urls(tweet_text):\n",
    "    tweet_text = re.sub('http[s]?:[\\\\\\\\]?/[\\\\\\\\]?/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', tweet_text)\n",
    "    tweet_text = re.sub('(@[A-Za-z0-9_]+)','', tweet_text)\n",
    "    tweet_text = re.sub('(#[A-Za-z0-9_]+)','', tweet_text)\n",
    "    return tweet_text.strip()\n",
    "    \n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?:[\\\\\\\\]?/[\\\\\\\\]?/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub('(@[A-Za-z0-9_]+)','', token)\n",
    "        token = re.sub('(#[A-Za-z0-9_]+)','', token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier on twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweets ands stop words\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize tweets\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove noise from tokens\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "# Get and show frequency distribution of positive words\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build positive and negative datasets\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 70/30 into train/validate, maintaining 50/50 positive/negative for both sets\n",
    "train_data = positive_dataset[:3500] + negative_dataset[:3500]\n",
    "validation_data = positive_dataset[3500:] + negative_dataset[3500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'top': True, 'engage': True, 'member': True, 'community': True, 'week': True, ':)': True}, 'Positive'), ({'hey': True, 'james': True, 'odd': True, ':/': True, 'please': True, 'call': True, 'contact': True, 'centre': True, '02392441234': True, 'able': True, 'assist': True, ':)': True, 'many': True, 'thanks': True}, 'Positive'), ({'listen': True, 'last': True, 'night': True, ':)': True, 'bleed': True, 'amazing': True, 'track': True, 'scotland': True}, 'Positive'), ({'congrats': True, ':)': True}, 'Positive'), ({'yeaaaah': True, 'yippppy': True, 'accnt': True, 'verify': True, 'rqst': True, 'succeed': True, 'get': True, 'blue': True, 'tick': True, 'mark': True, 'fb': True, 'profile': True, ':)': True, '15': True, 'day': True}, 'Positive'), ({'one': True, 'irresistible': True, ':)': True}, 'Positive'), ({'like': True, 'keep': True, 'lovely': True, 'customer': True, 'wait': True, 'long': True, 'hope': True, 'enjoy': True, 'happy': True, 'friday': True, 'lwwf': True, ':)': True}, 'Positive'), ({'second': True, 'thought': True, 'â€™': True, 'enough': True, 'time': True, 'dd': True, ':)': True, 'new': True, 'short': True, 'enter': True, 'system': True, 'sheep': True, 'must': True, 'buy': True}, 'Positive'), ({'jgh': True, 'go': True, 'bayan': True, ':d': True, 'bye': True}, 'Positive'), ({'act': True, 'mischievousness': True, 'call': True, 'etl': True, 'layer': True, 'in-house': True, 'warehouse': True, 'app': True, 'katamari': True, 'well': True, 'â€¦': True, 'name': True, 'imply': True, ':p': True}, 'Positive'), ({'top': True, 'influencers': True, 'community': True, 'week': True, ':)': True}, 'Positive'), ({'love': True, 'big': True, '...': True, 'juicy': True, 'selfies': True, ':)': True}, 'Positive'), ({'follow': True, 'u': True, 'back': True, ':)': True}, 'Positive'), ({'perfect': True, 'already': True, 'know': True, \"what's\": True, 'wait': True, ':)': True}, 'Positive'), ({'great': True, 'new': True, 'opportunity': True, 'junior': True, 'triathletes': True, 'age': True, '12': True, '13': True, 'gatorade': True, 'series': True, 'get': True, 'entry': True, ':)': True}, 'Positive'), ({'laying': True, 'greeting': True, 'card': True, 'range': True, 'print': True, 'today': True, 'love': True, 'job': True, ':-)': True}, 'Positive'), ({\"friend's\": True, 'lunch': True, '...': True, 'yummmm': True, ':)': True}, 'Positive'), ({'id': True, 'conflict': True, 'thanks': True, 'help': True, ':d': True, \"here's\": True, 'screenshot': True, 'work': True}, 'Positive'), ({'hi': True, 'liv': True, ':)': True}, 'Positive'), ({'hello': True, 'need': True, 'know': True, 'something': True, 'u': True, 'fm': True, 'twitter': True, 'â€”': True, 'sure': True, 'thing': True, ':)': True, 'dm': True, 'x': True}, 'Positive')]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9933333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2071.0 : 1.0\n",
      "                      :) = True           Positi : Negati =   1005.4 : 1.0\n",
      "                follower = True           Positi : Negati =     39.7 : 1.0\n",
      "                followed = True           Negati : Positi =     34.3 : 1.0\n",
      "                  arrive = True           Positi : Negati =     33.0 : 1.0\n",
      "                    glad = True           Positi : Negati =     23.7 : 1.0\n",
      "                     x15 = True           Negati : Positi =     23.7 : 1.0\n",
      "                     sad = True           Negati : Positi =     19.9 : 1.0\n",
      "                    sick = True           Negati : Positi =     19.7 : 1.0\n",
      "               community = True           Positi : Negati =     16.3 : 1.0\n",
      "                     ugh = True           Negati : Positi =     13.7 : 1.0\n",
      "                    miss = True           Negati : Positi =     12.7 : 1.0\n",
      "              definitely = True           Positi : Negati =     12.3 : 1.0\n",
      "                follback = True           Positi : Negati =     12.3 : 1.0\n",
      "                   sorry = True           Negati : Positi =     11.9 : 1.0\n",
      "                      aw = True           Negati : Positi =     11.7 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Train classifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, validation_data))\n",
    "print(classifier.show_most_informative_features(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision is:  0.9920212765957447\n",
      "Positive recall is:  0.9946666666666667\n",
      "Positive f-measure is:  0.9933422103861517\n",
      "Negative precision is:  0.9946524064171123\n",
      "Negative recall is:  0.992\n",
      "Negative f-measure is:  0.993324432576769\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "import collections\n",
    "\n",
    "refsets = collections.defaultdict(set)\n",
    "valsets = collections.defaultdict(set)\n",
    "for i, (feats, label) in enumerate(validation_data):\n",
    "    refsets[label].add(i)\n",
    "    observed = classifier.classify(feats)\n",
    "    valsets[observed].add(i)\n",
    "\n",
    "print(\"Positive precision is: \", precision(refsets['Positive'], valsets['Positive']))\n",
    "print(\"Positive recall is: \", recall(refsets['Positive'], valsets['Positive']))\n",
    "print(\"Positive f-measure is: \", f_measure(refsets['Positive'], valsets['Positive']))\n",
    "print(\"Negative precision is: \", precision(refsets['Negative'], valsets['Negative']))\n",
    "print(\"Negative recall is: \", recall(refsets['Negative'], valsets['Negative']))\n",
    "print(\"Negative f-measure is: \", f_measure(refsets['Negative'], valsets['Negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing classifier on custom tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:\t\"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
      "Class:\tNegative\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "print('Tweet:\\t\"%s\"\\nClass:\\t%s' % (custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying election tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweets into dataframe if there is no pickle available\n",
    "if path.exists('./full_df.pkl'):\n",
    "    df = pd.read_pickle('./full_df.pkl')\n",
    "else:\n",
    "    df=pd.read_json('geotagged_tweets_20160812-0912.jsons', lines=True)\n",
    "    df.to_pickle('./full_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563329, 9)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out tweets in other languages than English\n",
    "df_filtered = df[df['lang']=='en']\n",
    "# Select only the columns needed for analysis\n",
    "df_filtered = df_filtered[['id','text','in_reply_to_screen_name','quoted_status','source','geo','coordinates','place']]\n",
    "df_filtered = df_filtered.reset_index(drop=True)\n",
    "# Remove mentions, hashtags and links from text\n",
    "df_filtered['text_clean'] = df_filtered['text'].apply(lambda txt: remove_ats_hts_urls(txt))\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of classified tweets\n",
    "data = []\n",
    "for tweety in range(len(df_filtered['text'])):\n",
    "    # Extract some column values\n",
    "    txt = df_filtered['text'][tweety]\n",
    "    reply_to = df_filtered['in_reply_to_screen_name'][tweety]\n",
    "    quoted_status = df_filtered['quoted_status'][tweety]\n",
    "    # Determine the candidate the tweet is aimed at\n",
    "    candidate = determine_candidate(txt, reply_to, quoted_status)\n",
    "    # Use cleaned text for tokenizing\n",
    "    txt_clean = df_filtered['text_clean'][tweety]\n",
    "    # Only consider tweets that contain text other than mentions, hashtags and links\n",
    "    if len(txt_clean) > 0:\n",
    "        tweety_c = remove_noise(word_tokenize(txt_clean))\n",
    "        data.append([txt, tweety_c, classifier.classify(dict([token, True] for token in tweety_c)), candidate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into dataframe and analyse\n",
    "df_p_n = pd.DataFrame(data, columns =['Full tweet','Clean tweet tokens','Sentiment','Candidate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(563254, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Dataframe shape:\\n') \n",
    "df_p_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe head:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full tweet</th>\n",
       "      <th>Clean tweet tokens</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Candidate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...</td>\n",
       "      <td>[all, in, collusion, together]</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@HillaryClinton he will do in one year all the...</td>\n",
       "      <td>[he, will, do, in, one, year, all, the, thing,...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Clinton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#CNN #newday clear #Trump deliberately throwin...</td>\n",
       "      <td>[clear, deliberately, throw, this, race, in, 2...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@realDonaldTrump, you wouldn't recognize a lie...</td>\n",
       "      <td>[you, would, n't, recognize, a, lie, if, it, c...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Kid, you know, suing someone? Thats the most ...</td>\n",
       "      <td>[``, kid, you, know, sue, someone, thats, the,...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Trump</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Full tweet  \\\n",
       "0  @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "1  @HillaryClinton he will do in one year all the...   \n",
       "2  #CNN #newday clear #Trump deliberately throwin...   \n",
       "3  @realDonaldTrump, you wouldn't recognize a lie...   \n",
       "4  \"Kid, you know, suing someone? Thats the most ...   \n",
       "\n",
       "                                  Clean tweet tokens Sentiment Candidate  \n",
       "0                     [all, in, collusion, together]  Positive     Trump  \n",
       "1  [he, will, do, in, one, year, all, the, thing,...  Negative   Clinton  \n",
       "2  [clear, deliberately, throw, this, race, in, 2...  Positive     Trump  \n",
       "3  [you, would, n't, recognize, a, lie, if, it, c...  Negative     Trump  \n",
       "4  [``, kid, you, know, sue, someone, thats, the,...  Positive     Trump  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nDataframe head:\\n')\n",
    "df_p_n.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Kid, you know, suing someone? Thats the most beautiful thing 1 human being could do to another human being\" @funnyordie @realDonaldTrumpðŸ˜‚ðŸ’©s\n",
      "Tokens:  ['``', 'kid', 'you', 'know', 'sue', 'someone', 'thats', 'the', 'most', 'beautiful', 'thing', '1', 'human', 'be', 'could', 'do', 'to', 'another', 'human', 'be', \"''\", 'ðŸ˜‚ðŸ’©s']\n",
      "Sentiment:  Positive\n"
     ]
    }
   ],
   "source": [
    "print(df_p_n['Full tweet'][4])\n",
    "print('Tokens: ', df_p_n['Clean tweet tokens'][4])\n",
    "print('Sentiment: ', df_p_n['Sentiment'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe described:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                                                Full tweet  \\\n",
       "0       @BarackObama \\n@FBI\\n@LORETTALYNCH \\nALL IN CO...   \n",
       "1       @HillaryClinton he will do in one year all the...   \n",
       "2       #CNN #newday clear #Trump deliberately throwin...   \n",
       "3       @realDonaldTrump, you wouldn't recognize a lie...   \n",
       "4       \"Kid, you know, suing someone? Thats the most ...   \n",
       "...                                                   ...   \n",
       "563249  @CNBC @SquawkAlley @realDonaldTrump Kudlow is ...   \n",
       "563250  TRUMP U, TAXES ,WEIRD MEDICAL REPORT WITH A WH...   \n",
       "563251  @CarolCNN if MSM were honest watch any utube v...   \n",
       "563252  It's interesting that Hillary Clinton's crowds...   \n",
       "563253  @TeamTrump @KellyannePolls @realDonaldTrump @f...   \n",
       "\n",
       "                                       Clean tweet tokens Sentiment Candidate  \n",
       "0                          [all, in, collusion, together]  Positive     Trump  \n",
       "1       [he, will, do, in, one, year, all, the, thing,...  Negative   Clinton  \n",
       "2       [clear, deliberately, throw, this, race, in, 2...  Positive     Trump  \n",
       "3       [you, would, n't, recognize, a, lie, if, it, c...  Negative     Trump  \n",
       "4       [``, kid, you, know, sue, someone, thats, the,...  Positive     Trump  \n",
       "...                                                   ...       ...       ...  \n",
       "563249  [kudlow, be, on, tomorrow, .., what, will, he,...  Negative     Trump  \n",
       "563250  [trump, u, taxes, weird, medical, report, with...  Negative     Trump  \n",
       "563251  [if, msm, be, honest, watch, any, utube, video...  Positive     Trump  \n",
       "563252  [it, 's, interesting, that, hillary, clinton, ...  Positive     Trump  \n",
       "563253  [yep, include, the, deplorable, of, which, the...  Positive     Trump  \n",
       "\n",
       "[563254 rows x 4 columns]>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('\\nDataframe described:\\n')\n",
    "df_p_n.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(550435, 4)\n",
      "(12819, 4)\n"
     ]
    }
   ],
   "source": [
    "df_p_n_candidates = df_p_n[df_p_n['Candidate']!='Neither']\n",
    "df_p_n_neither = df_p_n[df_p_n['Candidate']=='Neither']\n",
    "print(df_p_n_candidates.shape)\n",
    "print(df_p_n_neither.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(197560, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p_n_candidates[(df_p_n_candidates['Candidate']=='Trump')&(df_p_n_candidates['Sentiment']=='Positive')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86005, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p_n_candidates[(df_p_n_candidates['Candidate']=='Clinton')&(df_p_n_candidates['Sentiment']=='Positive')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185487, 4)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p_n_candidates[(df_p_n_candidates['Candidate']=='Trump')&(df_p_n_candidates['Sentiment']=='Negative')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81383, 4)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p_n_candidates[(df_p_n_candidates['Candidate']=='Clinton')&(df_p_n_candidates['Sentiment']=='Negative')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6825, 4)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p_n_neither[(df_p_n_neither['Sentiment']=='Positive')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5994, 4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p_n_neither[(df_p_n_neither['Sentiment']=='Negative')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframes to pickle for further analyses\n",
    "df_p_n_candidates.to_pickle('./sentiment_analysis_w_candidate.pkl')\n",
    "df_p_n_neither.to_pickle('./sentiment_analysis_neither.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
